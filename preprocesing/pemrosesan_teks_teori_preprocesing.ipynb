{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpsIVNvuKCLtLUft2ENDB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tariqulq-cyber/Pemrosesan-teks-praktek/blob/main/preprocesing/pemrosesan_teks_teori_preprocesing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk Sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE3ZlcXJpkPv",
        "outputId": "46440459-5fc2-499c-8914-ea8d7f71516e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "metadata": {
        "id": "seS-AJcqoOXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c16d77a7-1709-446e-80a3-f59574af44b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt_tab\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package punkt_tab to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh18XwcPl6gN",
        "outputId": "1f2be690-9519-4d76-f7c4-8bcf9fbd514f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# =======================================================\n",
        "# 1. KONFIGURASI AWAL & SETUP LIBRARY\n",
        "# =======================================================\n",
        "\n",
        "NAMA_FILE_INPUT = '/content/drive/MyDrive/pemrosesan teks teori/casefolding_growtopia.csv' # File Input Anda\n",
        "NAMA_FILE_OUTPUT = 'preprocesing_growtopia.csv' # Nama file CSV output\n",
        "KOLOM_TEKS = 'content' # Nama kolom yang berisi teks\n",
        "\n",
        "# --- SETUP NLP ---\n",
        "# 1. Stopword Removal (Menggunakan list standar NLTK)\n",
        "list_stopwords = set(stopwords.words('indonesian'))\n",
        "# Tambahkan stopword informal yang sering muncul di komentar (PENTING)\n",
        "list_stopwords_tambahan = {\n",
        "    # Kata Ganti/Keterangan/Partikel Informal\n",
        "    'gua','itu','ini','yg','aja', 'sih', 'dong', 'deh', 'nya', 'nih', 'tuh', 'loh', 'kek',\n",
        "    'pake', 'msh', 'udh', 'emg', 'trs', 'dlm', 'sm', 'tp', 'bgt', 'gw','ni','apa','ya','yah','dh','dah','woi',\n",
        "\n",
        "    # Sapaan/Suku Kata/Singkatan\n",
        "    'cuy', 'min', 'admin', 'gan', 'sis', 'kak', 'kalo', 'klo', 'gt',\n",
        "    'gitu', 'sama', 'ama', 'bisa', 'kalian', 'mereka', 'sii', 'ah', 'eh',\n",
        "    'hmm', 'wkwk', 'hehe', 'hihi','sbg','dgn',\n",
        "\n",
        "    # Kata Kerja/Hubung Umum yang sering diulang\n",
        "    'adalah', 'iya', 'sudah', 'telah', 'mau', 'ingin', 'dan', 'di',\n",
        "    'growtopia', 'gsc' # Contoh kata spesifik dari dataset\n",
        "}\n",
        "list_stopwords = list_stopwords.union(list_stopwords_tambahan)\n",
        "print(f\"[INFO] Stopword berhasil digabungkan (UNION). Total: {len(list_stopwords)} kata.\")\n",
        "# 2. Stemming\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# --- KAMUS NORMALISASI KATA (Contoh) ---\n",
        "normalisasi_map = {\n",
        "    'gaje': 'tidak jelas',\n",
        "    'bgt': 'banget',\n",
        "    'bikin': 'membuat',\n",
        "    'ga': 'tidak',\n",
        "    'gk': 'tidak',\n",
        "    'gak': 'tidak',\n",
        "    'udh': 'sudah',\n",
        "    'mabar': 'main bareng',\n",
        "    'krn': 'karena',\n",
        "    'cpt': 'cepat',\n",
        "    'kzl': 'kesal',\n",
        "    'tp': 'tapi',\n",
        "    'anj': 'anjing',\n",
        "    'log': 'login',\n",
        "    'gem':'game'\n",
        "\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "# --- Fungsi Normalisasi Kata (SAMA) ---\n",
        "def normalisasi_kata(teks, mapping):\n",
        "    if pd.isna(teks): return \"\"\n",
        "    list_kata = teks.split()\n",
        "    kata_normal = [mapping.get(kata, kata) for kata in list_kata]\n",
        "    return \" \".join(kata_normal)\n",
        "\n",
        "print(\"=======================================================\")\n",
        "print(f\"Memulai Preprocessing NLP (Stopword Intensif) untuk file: {NAMA_FILE_INPUT}\")\n",
        "print(\"=======================================================\")\n",
        "\n",
        "# =======================================================\n",
        "# 2 & 3. MUAT DATA DAN HAPUS DUPLIKAT (SAMA)\n",
        "# =======================================================\n",
        "try:\n",
        "    df = pd.read_csv(NAMA_FILE_INPUT)\n",
        "    if KOLOM_TEKS not in df.columns:\n",
        "        print(f\"[ERROR] Kolom '{KOLOM_TEKS}' tidak ditemukan.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    df[KOLOM_TEKS] = df[KOLOM_TEKS].fillna('').astype(str).str.lower()\n",
        "    total_awal = len(df)\n",
        "    df.drop_duplicates(subset=[KOLOM_TEKS], inplace=True)\n",
        "    print(f\"[INFO] Data dimuat. Duplikat terhapus: {total_awal - len(df)} baris.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] Gagal memuat file CSV: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =======================================================\n",
        "# 4. RANTAI PREPROCESSING LENGKAP (MODIFIKASI STOPWORD)\n",
        "# =======================================================\n",
        "\n",
        "def preprocessing_lengkap(teks):\n",
        "    # 1. Normalisasi Kata (Slang/Tidak Baku)\n",
        "    teks = normalisasi_kata(teks, normalisasi_map)\n",
        "\n",
        "    # 2. Penghapusan Tanda Baca, Simbol, dan Angka (Cleaning)\n",
        "    teks = re.sub(r'[^a-zA-Z\\s]', '', teks)\n",
        "\n",
        "    # 3. Tokenisasi\n",
        "    tokens = word_tokenize(teks)\n",
        "\n",
        "    # 4. >>> STOPWORD REMOVAL STANDAR (HANYA MENGHILANGKAN SEMUA KATA FUNGSI) <<<\n",
        "    # Perhatikan: Tidak ada filter panjang kata <= 3 di sini\n",
        "    tokens_no_stop = [\n",
        "        word for word in tokens\n",
        "        if word not in list_stopwords and len(word) > 1 # Filter kata 1 huruf (biasanya sisa cleaning)\n",
        "    ]\n",
        "\n",
        "    # 5. Stemming (Mengembalikan ke kata dasar)\n",
        "    tokens_stemmed = [stemmer.stem(word) for word in tokens_no_stop]\n",
        "\n",
        "    # Menggabungkan kembali menjadi string bersih\n",
        "    return \" \".join(tokens_stemmed)\n",
        "\n",
        "print(\"\\n[INFO] Mulai menerapkan rantai Preprocessing Lengkap (Stopword Intensif)...\")\n",
        "df['content_processed_FINAL'] = df[KOLOM_TEKS].apply(preprocessing_lengkap)\n",
        "print(\"[INFO] Preprocessing Lengkap selesai.\")\n",
        "\n",
        "\n",
        "# =======================================================\n",
        "# 5. EKSPOR DATA KE CSV BARU (SAMA)\n",
        "# =======================================================\n",
        "\n",
        "df_export = df.copy()\n",
        "\n",
        "print(\"\\n[PREVIEW] Perbandingan data MENTAH vs. data AKHIR (5 Baris):\")\n",
        "print(df_export[[KOLOM_TEKS, 'content_processed_FINAL']].head().to_string(index=False))\n",
        "\n",
        "try:\n",
        "    kolom_final = list(df.columns)\n",
        "    with open(NAMA_FILE_OUTPUT, 'w', newline='', encoding='utf-8') as f:\n",
        "        df_export.to_csv(f, index=False, encoding='utf-8', columns=kolom_final)\n",
        "\n",
        "    print(f\"\\n[SUKSES EKSPOR] Data Preprocessing Lengkap disimpan ke file:\")\n",
        "    print(f\"{os.path.abspath(NAMA_FILE_OUTPUT)}\")\n",
        "    print(\"=======================================================\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"=======================================================\")\n",
        "    print(f\"\\n[!!! KEGAGALAN KRITIS EKSPOR CSV !!!] Tipe Kesalahan: {type(e).__name__}, Detail: {e}\")\n",
        "    print(\"=======================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IflK_lWt1-JO",
        "outputId": "0275dc3d-afe2-402f-a29c-04744d624950"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Stopword berhasil digabungkan (UNION). Total: 806 kata.\n",
            "=======================================================\n",
            "Memulai Preprocessing NLP (Stopword Intensif) untuk file: /content/drive/MyDrive/pemrosesan teks teori/casefolding_growtopia.csv\n",
            "=======================================================\n",
            "[INFO] Data dimuat. Duplikat terhapus: 50 baris.\n",
            "\n",
            "[INFO] Mulai menerapkan rantai Preprocessing Lengkap (Stopword Intensif)...\n",
            "[INFO] Preprocessing Lengkap selesai.\n",
            "\n",
            "[PREVIEW] Perbandingan data MENTAH vs. data AKHIR (5 Baris):\n",
            "                                                                                                                                                   content                                                                                                                         content_processed_FINAL\n",
            "                                                                              game nya susah banget anj cuman buat login bikin akun di persulit game konfl                                                                                      game susah banget anjing cuman login akun sulit game konfl\n",
            "                                                                                                                                                         🤑                                                                                                                                                \n",
            "                                                                                                                                                  not good                                                                                                                                        not good\n",
            "                                                                                                                                                     bagus                                                                                                                                           bagus\n",
            "i really love this game but i'm always disappointed with the system. i can't log in using data/wifi, it often disconnects, and i get banned for no reason. really love this game but im always disappointed with the system cant login in using datawifi it often disconnects and get banned for no reason\n",
            "\n",
            "[SUKSES EKSPOR] Data Preprocessing Lengkap disimpan ke file:\n",
            "/content/preprocesing_growtopia.csv\n",
            "=======================================================\n"
          ]
        }
      ]
    }
  ]
}