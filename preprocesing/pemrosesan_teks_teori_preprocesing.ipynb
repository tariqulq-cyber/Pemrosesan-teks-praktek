{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHRnLoE2W/oJHgBaN1hvdf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tariqulq-cyber/Pemrosesan-teks-praktek/blob/main/preprocesing/pemrosesan_teks_teori_preprocesing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk Sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE3ZlcXJpkPv",
        "outputId": "75839cff-7c78-4643-a5b1-93f7654a2ef5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "metadata": {
        "id": "seS-AJcqoOXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c55a4e-44fa-4f06-9b03-4d6e35b547be"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt_tab\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package punkt_tab to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> stopwords\n",
            "Command 'stopwords' unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh18XwcPl6gN",
        "outputId": "be8dd4d2-bea0-43f4-d385-1bd91195b06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# =======================================================\n",
        "# 1. KONFIGURASI AWAL & SETUP LIBRARY\n",
        "# =======================================================\n",
        "\n",
        "NAMA_FILE_INPUT = '/content/drive/MyDrive/pemrosesan teks teori/casefolding_growtopia.csv' # File Input Anda\n",
        "NAMA_FILE_OUTPUT = 'preprocesing_growtopia 2 .csv' # Nama file CSV output\n",
        "KOLOM_TEKS = 'content' # Nama kolom yang berisi teks\n",
        "\n",
        "# --- SETUP NLP ---\n",
        "# 1. Stopword Removal (Menggunakan list standar NLTK)\n",
        "list_stopwords = set(stopwords.words('indonesian'))\n",
        "# Tambahkan stopword informal yang sering muncul di komentar (PENTING)\n",
        "list_stopwords.update(['yg', 'dan', 'nya', 'aja', 'sih', 'dong', 'deh','di','growtopia','gt'])\n",
        "\n",
        "# 2. Stemming\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# --- KAMUS NORMALISASI KATA (Contoh) ---\n",
        "normalisasi_map = {\n",
        "    'gaje': 'tidak jelas',\n",
        "    'bgt': 'banget',\n",
        "    'bikin': 'membuat',\n",
        "    'ga': 'tidak',\n",
        "    'udh': 'sudah',\n",
        "    'mabar': 'main bareng',\n",
        "    'krn': 'karena',\n",
        "    'cpt': 'cepat',\n",
        "    'kzl': 'kesal',\n",
        "    'tp': 'tapi',\n",
        "    'anj': 'anjing'\n",
        "}\n",
        "\n",
        "# --- Fungsi Normalisasi Kata (SAMA) ---\n",
        "def normalisasi_kata(teks, mapping):\n",
        "    if pd.isna(teks): return \"\"\n",
        "    list_kata = teks.split()\n",
        "    kata_normal = [mapping.get(kata, kata) for kata in list_kata]\n",
        "    return \" \".join(kata_normal)\n",
        "\n",
        "print(\"=======================================================\")\n",
        "print(f\"Memulai Preprocessing NLP (Stopword Intensif) untuk file: {NAMA_FILE_INPUT}\")\n",
        "print(\"=======================================================\")\n",
        "\n",
        "# =======================================================\n",
        "# 2 & 3. MUAT DATA DAN HAPUS DUPLIKAT (SAMA)\n",
        "# =======================================================\n",
        "try:\n",
        "    df = pd.read_csv(NAMA_FILE_INPUT)\n",
        "    if KOLOM_TEKS not in df.columns:\n",
        "        print(f\"[ERROR] Kolom '{KOLOM_TEKS}' tidak ditemukan.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    df[KOLOM_TEKS] = df[KOLOM_TEKS].fillna('').astype(str).str.lower()\n",
        "    total_awal = len(df)\n",
        "    df.drop_duplicates(subset=[KOLOM_TEKS], inplace=True)\n",
        "    print(f\"[INFO] Data dimuat. Duplikat terhapus: {total_awal - len(df)} baris.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] Gagal memuat file CSV: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# =======================================================\n",
        "# 4. RANTAI PREPROCESSING LENGKAP (MODIFIKASI STOPWORD)\n",
        "# =======================================================\n",
        "\n",
        "def preprocessing_lengkap(teks):\n",
        "    # 1. Normalisasi Kata (Slang/Tidak Baku)\n",
        "    teks = normalisasi_kata(teks, normalisasi_map)\n",
        "\n",
        "    # 2. Penghapusan Tanda Baca, Simbol, dan Angka (Cleaning)\n",
        "    teks = re.sub(r'[^a-zA-Z\\s]', '', teks)\n",
        "\n",
        "    # 3. Tokenisasi\n",
        "    tokens = word_tokenize(teks)\n",
        "\n",
        "    # 4. >>> STOPWORD REMOVAL STANDAR (HANYA MENGHILANGKAN SEMUA KATA FUNGSI) <<<\n",
        "    # Perhatikan: Tidak ada filter panjang kata <= 3 di sini\n",
        "    tokens_no_stop = [\n",
        "        word for word in tokens\n",
        "        if word not in list_stopwords and len(word) > 3 # Filter kata 1 huruf (biasanya sisa cleaning)\n",
        "    ]\n",
        "\n",
        "    # 5. Stemming (Mengembalikan ke kata dasar)\n",
        "    tokens_stemmed = [stemmer.stem(word) for word in tokens_no_stop]\n",
        "\n",
        "    # Menggabungkan kembali menjadi string bersih\n",
        "    return \" \".join(tokens_stemmed)\n",
        "\n",
        "print(\"\\n[INFO] Mulai menerapkan rantai Preprocessing Lengkap (Stopword Intensif)...\")\n",
        "df['content_processed_FINAL'] = df[KOLOM_TEKS].apply(preprocessing_lengkap)\n",
        "print(\"[INFO] Preprocessing Lengkap selesai.\")\n",
        "\n",
        "\n",
        "# =======================================================\n",
        "# 5. EKSPOR DATA KE CSV BARU (SAMA)\n",
        "# =======================================================\n",
        "\n",
        "df_export = df.copy()\n",
        "\n",
        "print(\"\\n[PREVIEW] Perbandingan data MENTAH vs. data AKHIR (5 Baris):\")\n",
        "print(df_export[[KOLOM_TEKS, 'content_processed_FINAL']].head().to_string(index=False))\n",
        "\n",
        "try:\n",
        "    kolom_final = list(df.columns)\n",
        "    with open(NAMA_FILE_OUTPUT, 'w', newline='', encoding='utf-8') as f:\n",
        "        df_export.to_csv(f, index=False, encoding='utf-8', columns=kolom_final)\n",
        "\n",
        "    print(f\"\\n[SUKSES EKSPOR] Data Preprocessing Lengkap disimpan ke file:\")\n",
        "    print(f\"{os.path.abspath(NAMA_FILE_OUTPUT)}\")\n",
        "    print(\"=======================================================\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"=======================================================\")\n",
        "    print(f\"\\n[!!! KEGAGALAN KRITIS EKSPOR CSV !!!] Tipe Kesalahan: {type(e).__name__}, Detail: {e}\")\n",
        "    print(\"=======================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IflK_lWt1-JO",
        "outputId": "f3c94c33-cd7e-4787-e7b3-1dc6f779191a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "Memulai Preprocessing NLP (Stopword Intensif) untuk file: /content/drive/MyDrive/pemrosesan teks teori/casefolding_growtopia.csv\n",
            "=======================================================\n",
            "[INFO] Data dimuat. Duplikat terhapus: 50 baris.\n",
            "\n",
            "[INFO] Mulai menerapkan rantai Preprocessing Lengkap (Stopword Intensif)...\n",
            "[INFO] Preprocessing Lengkap selesai.\n",
            "\n",
            "[PREVIEW] Perbandingan data MENTAH vs. data AKHIR (5 Baris):\n",
            "                                                                                                                                                   content                                                                                   content_processed_FINAL\n",
            "                                                                              game nya susah banget anj cuman buat login bikin akun di persulit game konfl                                                game susah banget anjing cuman login akun sulit game konfl\n",
            "                                                                                                                                                         ðŸ¤‘                                                                                                          \n",
            "                                                                                                                                                  not good                                                                                                      good\n",
            "                                                                                                                                                     bagus                                                                                                     bagus\n",
            "i really love this game but i'm always disappointed with the system. i can't log in using data/wifi, it often disconnects, and i get banned for no reason. really love this game always disappointed with system cant using datawifi often disconnects banned reason\n",
            "\n",
            "[SUKSES EKSPOR] Data Preprocessing Lengkap disimpan ke file:\n",
            "/content/preprocesing_growtopia 2 .csv\n",
            "=======================================================\n"
          ]
        }
      ]
    }
  ]
}